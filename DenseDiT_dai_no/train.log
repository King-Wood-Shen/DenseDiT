Using config: train/config/config_densedit_stage2.yaml
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W1004 23:10:17.539000 205719 site-packages/torch/distributed/run.py:774] 
W1004 23:10:17.539000 205719 site-packages/torch/distributed/run.py:774] *****************************************
W1004 23:10:17.539000 205719 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1004 23:10:17.539000 205719 site-packages/torch/distributed/run.py:774] *****************************************
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
================================================================================
Training Configuration:
  Rank: 0/2
  Run name: 20251004-231028
  Model path: /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext
  Model dtype: float32
  Mixed precision: bfloat16
  Batch size per GPU: 1
  Gradient accumulation: 4
  Effective batch size: 8
  DeepSpeed ZeRO Stage: 2
  Offload strategy: none
  Training precision: BF16 mixed
  Gradient checkpointing: True
================================================================================
Dataset length: 50
Initializing DenseDiTModel...
  dtype: torch.float32
  gradient_checkpointing: True
Loading FluxKontext pipeline from /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext...
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Initializing DenseDiTModel...
  dtype: torch.float32
  gradient_checkpointing: True
Loading FluxKontext pipeline from /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext...
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:00<00:01,  2.51it/s]`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][ALoading pipeline components...:  14%|â–ˆâ–        | 1/7 [00:00<00:01,  4.58it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading pipeline components...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:00<00:00,  6.00it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading pipeline components...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:00<00:00,  4.19it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.16s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.41s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.37s/it]
Loading pipeline components...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:07<00:12,  3.08s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:08<00:16,  8.10s/it][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:12<00:24, 12.06s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:19<00:10, 10.28s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  7.74s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.20s/it]
Loading pipeline components...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:25<00:16,  8.35s/it]Loading pipeline components...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:25<00:05,  5.71s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:26<00:13, 13.42s/it][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.03s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:33<00:00, 10.37s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:33<00:00, 11.06s/it]
Loading pipeline components...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:40<00:43, 14.37s/it]Loading pipeline components...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:41<00:19,  9.61s/it]Loading pipeline components...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:42<00:06,  6.61s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.12s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.10s/it]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  9.04s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  6.01s/it]
  Gradient checkpointing enabled
Freezing non-trainable components...
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:43<00:00,  4.75s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:43<00:00,  6.15s/it]
  Gradient checkpointing enabled
Freezing non-trainable components...
Setting up trainable parameters...
Trainable parameters: 6,456,794,624 / 11,901,408,320 (54.25%)
DenseDiTModel initialized successfully
Model parameter on  cpu
Starting training with DeepSpeed ZeRO Stage 2 (offload: none)...
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2
Setting up trainable parameters...
Trainable parameters: 6,456,794,624 / 11,901,408,320 (54.25%)
DenseDiTModel initialized successfully
Model parameter on  cpu
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
Starting training with DeepSpeed ZeRO Stage 2 (offload: none)...
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2
70e00f7e8c2a:205816:205816 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:205816:205816 [0] NCCL INFO Bootstrap: Using eth0:172.17.0.2<0>
70e00f7e8c2a:205816:205816 [0] NCCL INFO cudaDriverVersion 12080
70e00f7e8c2a:205816:205816 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
70e00f7e8c2a:205817:205817 [1] NCCL INFO cudaDriverVersion 12080
70e00f7e8c2a:205816:205816 [0] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:205817:205817 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:205817:205817 [1] NCCL INFO Bootstrap: Using eth0:172.17.0.2<0>
70e00f7e8c2a:205817:205817 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
70e00f7e8c2a:205817:205817 [1] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:205817:206689 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
70e00f7e8c2a:205817:206689 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
70e00f7e8c2a:205817:206689 [1] NCCL INFO Failed to open libibverbs.so[.1]
70e00f7e8c2a:205817:206689 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:205817:206689 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
70e00f7e8c2a:205817:206689 [1] NCCL INFO Initialized NET plugin Socket
70e00f7e8c2a:205817:206689 [1] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:205817:206689 [1] NCCL INFO Using network Socket
70e00f7e8c2a:205817:206689 [1] NCCL INFO ncclCommInitRankConfig comm 0x15c0b540 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0xf07ea4466726554f - Init START
70e00f7e8c2a:205816:206688 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
70e00f7e8c2a:205816:206688 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
70e00f7e8c2a:205816:206688 [0] NCCL INFO Failed to open libibverbs.so[.1]
70e00f7e8c2a:205816:206688 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:205816:206688 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
70e00f7e8c2a:205816:206688 [0] NCCL INFO Initialized NET plugin Socket
70e00f7e8c2a:205816:206688 [0] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:205816:206688 [0] NCCL INFO Using network Socket
70e00f7e8c2a:205816:206688 [0] NCCL INFO ncclCommInitRankConfig comm 0x343a6000 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0xf07ea4466726554f - Init START
70e00f7e8c2a:205816:206688 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
70e00f7e8c2a:205817:206689 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
70e00f7e8c2a:205816:206688 [0] NCCL INFO Bootstrap timings total 0.039345 (create 0.000060, send 0.018164, recv 0.000304, ring 0.020166, delay 0.000002)
70e00f7e8c2a:205817:206689 [1] NCCL INFO Bootstrap timings total 0.052681 (create 0.000038, send 0.000177, recv 0.030842, ring 0.000822, delay 0.000002)
70e00f7e8c2a:205816:206688 [0] NCCL INFO Setting affinity for GPU 0 to 0-7,64-71
70e00f7e8c2a:205817:206689 [1] NCCL INFO Setting affinity for GPU 1 to 0-7,64-71
70e00f7e8c2a:205817:206689 [1] NCCL INFO comm 0x15c0b540 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
70e00f7e8c2a:205817:206689 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
70e00f7e8c2a:205817:206689 [1] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:205817:206689 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
70e00f7e8c2a:205817:206692 [1] NCCL INFO [Proxy Service] Device 1 CPU core 7
70e00f7e8c2a:205817:206693 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 70
70e00f7e8c2a:205816:206688 [0] NCCL INFO comm 0x343a6000 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
70e00f7e8c2a:205816:206688 [0] NCCL INFO Channel 00/04 : 0 1
70e00f7e8c2a:205816:206688 [0] NCCL INFO Channel 01/04 : 0 1
70e00f7e8c2a:205816:206688 [0] NCCL INFO Channel 02/04 : 0 1
70e00f7e8c2a:205816:206688 [0] NCCL INFO Channel 03/04 : 0 1
70e00f7e8c2a:205816:206688 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
70e00f7e8c2a:205816:206688 [0] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:205816:206688 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
70e00f7e8c2a:205816:206688 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
70e00f7e8c2a:205816:206694 [0] NCCL INFO [Proxy Service] Device 0 CPU core 68
70e00f7e8c2a:205816:206695 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 70
70e00f7e8c2a:205817:206689 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:205817:206689 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:205816:206688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:205816:206688 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:205816:206688 [0] NCCL INFO CC Off, workFifoBytes 1048576
70e00f7e8c2a:205816:206688 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
70e00f7e8c2a:205817:206689 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
70e00f7e8c2a:205816:206688 [0] NCCL INFO ncclCommInitRankConfig comm 0x343a6000 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0xf07ea4466726554f - Init COMPLETE
70e00f7e8c2a:205817:206689 [1] NCCL INFO ncclCommInitRankConfig comm 0x15c0b540 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0xf07ea4466726554f - Init COMPLETE
70e00f7e8c2a:205816:206688 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.26 (kernels 0.19, alloc 0.01, bootstrap 0.04, allgathers 0.01, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
70e00f7e8c2a:205817:206689 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.26 (kernels 0.18, alloc 0.00, bootstrap 0.05, allgathers 0.01, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
70e00f7e8c2a:205817:206696 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:206696 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:206696 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:206696 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205816:206697 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:206697 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:206697 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:206697 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205817:206696 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
70e00f7e8c2a:205816:206697 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
Configuring optimizers...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Number of trainable parameters: 608
Optimizer configured: AdamW
Configuring optimizers...
Number of trainable parameters: 608
Optimizer configured: AdamW
70e00f7e8c2a:205816:205816 [0] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:205817:205817 [1] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:205817:207133 [1] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:205817:207133 [1] NCCL INFO Using network Socket
70e00f7e8c2a:205816:207107 [0] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:205816:207107 [0] NCCL INFO Using network Socket
70e00f7e8c2a:205817:207133 [1] NCCL INFO ncclCommSplit comm 0x1249ac00 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 parent 0x15c0b540 splitCount 1 color 2130503744 key 1- Init START
70e00f7e8c2a:205816:207107 [0] NCCL INFO ncclCommSplit comm 0x3b426650 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 parent 0x343a6000 splitCount 1 color 2130503744 key 0- Init START
70e00f7e8c2a:205817:207133 [1] NCCL INFO Setting affinity for GPU 1 to 0-7,64-71
70e00f7e8c2a:205816:207107 [0] NCCL INFO Setting affinity for GPU 0 to 0-7,64-71
70e00f7e8c2a:205816:207107 [0] NCCL INFO comm 0x3b426650 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
70e00f7e8c2a:205817:207133 [1] NCCL INFO comm 0x1249ac00 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
70e00f7e8c2a:205816:207107 [0] NCCL INFO Channel 00/04 : 0 1
70e00f7e8c2a:205817:207133 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
70e00f7e8c2a:205816:207107 [0] NCCL INFO Channel 01/04 : 0 1
70e00f7e8c2a:205817:207133 [1] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:205816:207107 [0] NCCL INFO Channel 02/04 : 0 1
70e00f7e8c2a:205816:207107 [0] NCCL INFO Channel 03/04 : 0 1
70e00f7e8c2a:205816:207107 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
70e00f7e8c2a:205816:207107 [0] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:205816:207107 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
70e00f7e8c2a:205817:207134 [1] NCCL INFO [Proxy Service] Device 1 CPU core 6
70e00f7e8c2a:205817:207136 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 68
70e00f7e8c2a:205816:207135 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
70e00f7e8c2a:205816:207137 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
70e00f7e8c2a:205817:207133 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:205817:207133 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:205816:207107 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:205816:207107 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:205816:207107 [0] NCCL INFO CC Off, workFifoBytes 1048576
70e00f7e8c2a:205817:207133 [1] NCCL INFO ncclCommSplit comm 0x1249ac00 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 parent 0x15c0b540 splitCount 1 color 2130503744 key 1 - Init COMPLETE
70e00f7e8c2a:205816:207107 [0] NCCL INFO ncclCommSplit comm 0x3b426650 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 parent 0x343a6000 splitCount 1 color 2130503744 key 0 - Init COMPLETE
70e00f7e8c2a:205817:207133 [1] NCCL INFO Init timings - ncclCommSplit: rank 1 nranks 2 total 0.02 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
70e00f7e8c2a:205816:207107 [0] NCCL INFO Init timings - ncclCommSplit: rank 0 nranks 2 total 1.87 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 1.86)
70e00f7e8c2a:205817:207138 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:207138 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:207138 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205817:207138 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:205816:207139 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:207139 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:207139 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205816:207139 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:205817:207138 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
70e00f7e8c2a:205816:207139 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name        | Type                   | Params | Mode 
---------------------------------------------------------------
0 | transformer | FluxTransformer2DModel | 11.9 B | train
---------------------------------------------------------------
6.5 B     Trainable params
5.4 B     Non-trainable params
11.9 B    Total params
47,605.633Total estimated model params size (MB)
1279      Modules in train mode
0         Modules in eval mode
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'train_dtype' parameter because it is not possible to safely dump to YAML.
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'inference_dtype' parameter because it is not possible to safely dump to YAML.
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/25 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
[rank0]:   File "<frozen runpy>", line 88, in _run_code
[rank0]:   File "/root/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 331, in <module>
[rank0]:     main()
[rank0]:   File "/root/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 327, in main
[rank0]:     trainer.fit(trainable_model, train_loader)  # å–æ¶ˆæ³¨é‡Šä»¥å¼€å§‹è®­ç»ƒ
[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 48, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1011, in _run
[rank0]:     results = self._run_stage()
[rank0]:               ^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py", line 1055, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank0]:     self.advance()
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py", line 458, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 152, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 348, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 192, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 270, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 177, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/module.py", line 1366, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py", line 154, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/ddp.py", line 273, in optimizer_step
[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 239, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/deepspeed.py", line 129, in optimizer_step
[rank0]:     closure_result = closure()
[rank0]:                      ^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 146, in __call__
[rank0]:     self._result = self.closure(*args, **kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 131, in closure
[rank0]:     step_output = self._step_fn()
[rank0]:                   ^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py", line 319, in _training_step
[rank0]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py", line 329, in _call_strategy_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 390, in training_step
[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2151, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
[rank0]:     out = method(*_args, **_kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 145, in training_step
[rank0]:     step_loss = self.step(batch)
[rank0]:                 ^^^^^^^^^^^^^^^^
[rank0]:   File "/root/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 197, in step
[rank0]:     latent_model_input = torch.cat([x_t, condition_latents, context_latents], dim=2)
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: Tensors must have same number of dimensions: got 4 and 3
[rank0]:[W1004 23:34:21.707417881 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:64 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:81 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:863 -> 3
70e00f7e8c2a:205816:206694 [0] NCCL INFO misc/socket.cc:915 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:64 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:81 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO misc/socket.cc:863 -> 3
70e00f7e8c2a:205816:215020 [0] NCCL INFO comm 0x343a6000 rank 0 nranks 2 cudaDev 0 busId 5000 - Abort COMPLETE
70e00f7e8c2a:205817:207134 [1] NCCL INFO [Service thread] Connection closed by localRank 0
W1004 23:34:25.783000 205719 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 205817 closing signal SIGTERM
[rank: 1] Received SIGTERM: 15
W1004 23:34:55.807000 205719 site-packages/torch/distributed/elastic/multiprocessing/api.py:919] Unable to shutdown process 205817 via 15, forcefully exiting via 9
E1004 23:34:56.737000 205719 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 205816) of binary: /opt/conda/envs/torch-env/bin/python3.12
Traceback (most recent call last):
  File "/opt/conda/envs/torch-env/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src.train.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-04_23:34:25
  host      : 70e00f7e8c2a
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 205816)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
