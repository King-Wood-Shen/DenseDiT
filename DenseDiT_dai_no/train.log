Using config: train/config/config_densedit_stage2.yaml
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W1005 15:01:09.258000 238609 site-packages/torch/distributed/run.py:774] 
W1005 15:01:09.258000 238609 site-packages/torch/distributed/run.py:774] *****************************************
W1005 15:01:09.258000 238609 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1005 15:01:09.258000 238609 site-packages/torch/distributed/run.py:774] *****************************************
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
================================================================================
Training Configuration:
  Rank: 0/2
  Run name: 20251005-150120
  Model path: /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext
  Model dtype: float32
  Mixed precision: bfloat16
  Batch size per GPU: 1
  Gradient accumulation: 4
  Effective batch size: 8
  DeepSpeed ZeRO Stage: 2
  Offload strategy: none
  Training precision: BF16 mixed
  Gradient checkpointing: True
================================================================================
Dataset length: 50
Initializing DenseDiTModel...
  dtype: torch.float32
  gradient_checkpointing: True
Loading FluxKontext pipeline from /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext...
Initializing DenseDiTModel...
  dtype: torch.float32
  gradient_checkpointing: True
Loading FluxKontext pipeline from /root/siton-data-51d3ce9aba3246f88f64ea65f79d5133/models/Flux-Kontext...
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  3.20it/s]Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:02,  2.97it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.71s/it][A
Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:21, 10.78s/it][A
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.25s/it][A
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.28s/it][A
Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  8.94s/it][ALoading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.68s/it]
Loading pipeline components...:  29%|██▊       | 2/7 [00:29<01:26, 17.31s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  8.96s/it][ALoading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.70s/it]
Loading pipeline components...:  29%|██▊       | 2/7 [00:29<01:26, 17.35s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading pipeline components...:  43%|████▎     | 3/7 [00:29<00:37,  9.46s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][ALoading pipeline components...:  43%|████▎     | 3/7 [00:30<00:38,  9.62s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.83s/it][A
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.70s/it][A
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.47s/it][ALoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.65s/it]
Loading pipeline components...:  71%|███████▏  | 5/7 [00:43<00:15,  7.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.68s/it][ALoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.86s/it]
Loading pipeline components...:  57%|█████▋    | 4/7 [00:43<00:33, 11.18s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading pipeline components...:  86%|████████▌ | 6/7 [00:43<00:05,  5.71s/it]Loading pipeline components...:  71%|███████▏  | 5/7 [00:43<00:14,  7.29s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:43<00:00,  4.13s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:43<00:00,  6.26s/it]
  Gradient checkpointing enabled
Freezing non-trainable components...
Loading pipeline components...: 100%|██████████| 7/7 [00:44<00:00,  3.67s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:44<00:00,  6.31s/it]
  Gradient checkpointing enabled
Freezing non-trainable components...
Setting up trainable parameters...
Trainable parameters: 6,456,794,624 / 11,901,408,320 (54.25%)
DenseDiTModel initialized successfully
Model parameter on  cpu
Starting training with DeepSpeed ZeRO Stage 2 (offload: none)...
Setting up trainable parameters...
Trainable parameters: 6,456,794,624 / 11,901,408,320 (54.25%)
DenseDiTModel initialized successfully
Model parameter on  cpu
💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
Starting training with DeepSpeed ZeRO Stage 2 (offload: none)...
You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2
70e00f7e8c2a:238689:238689 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:238689:238689 [0] NCCL INFO Bootstrap: Using eth0:172.17.0.2<0>
70e00f7e8c2a:238689:238689 [0] NCCL INFO cudaDriverVersion 12080
70e00f7e8c2a:238689:238689 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
70e00f7e8c2a:238689:238689 [0] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:238690:238690 [1] NCCL INFO cudaDriverVersion 12080
70e00f7e8c2a:238690:238690 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:238690:238690 [1] NCCL INFO Bootstrap: Using eth0:172.17.0.2<0>
70e00f7e8c2a:238690:238690 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
70e00f7e8c2a:238690:238690 [1] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:238689:238956 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
70e00f7e8c2a:238689:238956 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
70e00f7e8c2a:238689:238956 [0] NCCL INFO Failed to open libibverbs.so[.1]
70e00f7e8c2a:238689:238956 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:238689:238956 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
70e00f7e8c2a:238689:238956 [0] NCCL INFO Initialized NET plugin Socket
70e00f7e8c2a:238689:238956 [0] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:238689:238956 [0] NCCL INFO Using network Socket
70e00f7e8c2a:238689:238956 [0] NCCL INFO ncclCommInitRankConfig comm 0x14d596c0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0x262365bd241fd2a6 - Init START
70e00f7e8c2a:238690:238957 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. 
70e00f7e8c2a:238690:238957 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
70e00f7e8c2a:238690:238957 [1] NCCL INFO Failed to open libibverbs.so[.1]
70e00f7e8c2a:238690:238957 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
70e00f7e8c2a:238690:238957 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
70e00f7e8c2a:238690:238957 [1] NCCL INFO Initialized NET plugin Socket
70e00f7e8c2a:238690:238957 [1] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:238690:238957 [1] NCCL INFO Using network Socket
70e00f7e8c2a:238690:238957 [1] NCCL INFO ncclCommInitRankConfig comm 0x147bbf80 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0x262365bd241fd2a6 - Init START
70e00f7e8c2a:238690:238957 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
70e00f7e8c2a:238689:238956 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
70e00f7e8c2a:238689:238956 [0] NCCL INFO Bootstrap timings total 0.026487 (create 0.000035, send 0.010021, recv 0.000313, ring 0.015502, delay 0.000002)
70e00f7e8c2a:238690:238957 [1] NCCL INFO Bootstrap timings total 0.018980 (create 0.000034, send 0.000156, recv 0.002049, ring 0.000564, delay 0.000002)
70e00f7e8c2a:238689:238956 [0] NCCL INFO Setting affinity for GPU 0 to 0-7,64-71
70e00f7e8c2a:238690:238957 [1] NCCL INFO Setting affinity for GPU 1 to 0-7,64-71
70e00f7e8c2a:238689:238956 [0] NCCL INFO comm 0x14d596c0 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
70e00f7e8c2a:238689:238956 [0] NCCL INFO Channel 00/04 : 0 1
70e00f7e8c2a:238689:238956 [0] NCCL INFO Channel 01/04 : 0 1
70e00f7e8c2a:238689:238956 [0] NCCL INFO Channel 02/04 : 0 1
70e00f7e8c2a:238689:238956 [0] NCCL INFO Channel 03/04 : 0 1
70e00f7e8c2a:238689:238956 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
70e00f7e8c2a:238689:238956 [0] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:238690:238957 [1] NCCL INFO comm 0x147bbf80 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
70e00f7e8c2a:238689:238956 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
70e00f7e8c2a:238689:238956 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
70e00f7e8c2a:238690:238957 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
70e00f7e8c2a:238690:238957 [1] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:238689:238960 [0] NCCL INFO [Proxy Service] Device 0 CPU core 71
70e00f7e8c2a:238689:238961 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 68
70e00f7e8c2a:238690:238957 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
70e00f7e8c2a:238690:238962 [1] NCCL INFO [Proxy Service] Device 1 CPU core 68
70e00f7e8c2a:238690:238963 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 71
70e00f7e8c2a:238690:238957 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:238690:238957 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:238689:238956 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:238689:238956 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:238689:238956 [0] NCCL INFO CC Off, workFifoBytes 1048576
70e00f7e8c2a:238689:238956 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
70e00f7e8c2a:238690:238957 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
70e00f7e8c2a:238689:238956 [0] NCCL INFO ncclCommInitRankConfig comm 0x14d596c0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 commId 0x262365bd241fd2a6 - Init COMPLETE
70e00f7e8c2a:238690:238957 [1] NCCL INFO ncclCommInitRankConfig comm 0x147bbf80 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 commId 0x262365bd241fd2a6 - Init COMPLETE
70e00f7e8c2a:238689:238956 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.24 (kernels 0.19, alloc 0.00, bootstrap 0.03, allgathers 0.01, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
70e00f7e8c2a:238690:238957 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.24 (kernels 0.19, alloc 0.00, bootstrap 0.02, allgathers 0.01, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
70e00f7e8c2a:238690:238964 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238690:238964 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238690:238964 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238690:238964 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238689:238965 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238689:238965 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238689:238965 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238689:238965 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238689:238965 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
70e00f7e8c2a:238690:238964 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Configuring optimizers...
Configuring optimizers...
Number of trainable parameters: 608
Number of trainable parameters: 608
Optimizer configured: AdamW
Optimizer configured: AdamW
70e00f7e8c2a:238690:238690 [1] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:238689:238689 [0] NCCL INFO Comm config Blocking set to 1
70e00f7e8c2a:238689:238986 [0] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:238689:238986 [0] NCCL INFO Using network Socket
70e00f7e8c2a:238690:238968 [1] NCCL INFO Assigned NET plugin Socket to comm
70e00f7e8c2a:238690:238968 [1] NCCL INFO Using network Socket
70e00f7e8c2a:238690:238968 [1] NCCL INFO ncclCommSplit comm 0x149fc000 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 parent 0x147bbf80 splitCount 1 color 2130503744 key 1- Init START
70e00f7e8c2a:238689:238986 [0] NCCL INFO ncclCommSplit comm 0x1a999cc0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 parent 0x14d596c0 splitCount 1 color 2130503744 key 0- Init START
70e00f7e8c2a:238690:238968 [1] NCCL INFO Setting affinity for GPU 1 to 0-7,64-71
70e00f7e8c2a:238689:238986 [0] NCCL INFO Setting affinity for GPU 0 to 0-7,64-71
70e00f7e8c2a:238689:238986 [0] NCCL INFO comm 0x1a999cc0 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
70e00f7e8c2a:238690:238968 [1] NCCL INFO comm 0x149fc000 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
70e00f7e8c2a:238689:238986 [0] NCCL INFO Channel 00/04 : 0 1
70e00f7e8c2a:238689:238986 [0] NCCL INFO Channel 01/04 : 0 1
70e00f7e8c2a:238690:238968 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
70e00f7e8c2a:238689:238986 [0] NCCL INFO Channel 02/04 : 0 1
70e00f7e8c2a:238690:238968 [1] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:238689:238986 [0] NCCL INFO Channel 03/04 : 0 1
70e00f7e8c2a:238689:238986 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
70e00f7e8c2a:238689:238986 [0] NCCL INFO P2P Chunksize set to 131072
70e00f7e8c2a:238689:238986 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
70e00f7e8c2a:238689:238988 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
70e00f7e8c2a:238689:238989 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 70
70e00f7e8c2a:238690:238987 [1] NCCL INFO [Proxy Service] Device 1 CPU core 4
70e00f7e8c2a:238690:238990 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 70
70e00f7e8c2a:238689:238986 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:238689:238986 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:238690:238968 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
70e00f7e8c2a:238690:238968 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
70e00f7e8c2a:238689:238986 [0] NCCL INFO CC Off, workFifoBytes 1048576
70e00f7e8c2a:238689:238986 [0] NCCL INFO ncclCommSplit comm 0x1a999cc0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 5000 parent 0x14d596c0 splitCount 1 color 2130503744 key 0 - Init COMPLETE
70e00f7e8c2a:238690:238968 [1] NCCL INFO ncclCommSplit comm 0x149fc000 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 6000 parent 0x147bbf80 splitCount 1 color 2130503744 key 1 - Init COMPLETE
70e00f7e8c2a:238689:238986 [0] NCCL INFO Init timings - ncclCommSplit: rank 0 nranks 2 total 0.02 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
70e00f7e8c2a:238690:238968 [1] NCCL INFO Init timings - ncclCommSplit: rank 1 nranks 2 total 3.62 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 3.61)
70e00f7e8c2a:238690:238991 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238690:238991 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238689:238992 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238690:238991 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238689:238992 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238690:238991 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
70e00f7e8c2a:238689:238992 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238689:238992 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
70e00f7e8c2a:238690:238991 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
70e00f7e8c2a:238689:238992 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name        | Type                   | Params | Mode 
---------------------------------------------------------------
0 | transformer | FluxTransformer2DModel | 11.9 B | train
---------------------------------------------------------------
6.5 B     Trainable params
5.4 B     Non-trainable params
11.9 B    Total params
47,605.633Total estimated model params size (MB)
1279      Modules in train mode
0         Modules in eval mode
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'train_dtype' parameter because it is not possible to safely dump to YAML.
/opt/conda/envs/torch-env/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'inference_dtype' parameter because it is not possible to safely dump to YAML.
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]Input shapes latent_model_input: torch.Size([1, 768, 64]) prompt_embeds: torch.Size([1, 512, 4096]) pooled_prompt_embeds: torch.Size([1, 768]) text_ids: torch.Size([512, 3]) latent_ids: torch.Size([768, 3])
Input requires grad latent_model_input: False prompt_embeds: False pooled_prompt_embeds: False text_ids: False latent_ids: False
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Input shapes latent_model_input: torch.Size([1, 768, 64]) prompt_embeds: torch.Size([1, 512, 4096]) pooled_prompt_embeds: torch.Size([1, 768]) text_ids: torch.Size([512, 3]) latent_ids: torch.Size([768, 3])
Input requires grad latent_model_input: False prompt_embeds: False pooled_prompt_embeds: False text_ids: False latent_ids: False
/opt/conda/envs/torch-env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
Epoch 0:   4%|▍         | 1/25 [21:17<8:31:04,  0.00it/s]Epoch 0:   4%|▍         | 1/25 [21:17<8:31:04,  0.00it/s, v_num=29, train/loss=0.653, train/avg_timestep=0.575]