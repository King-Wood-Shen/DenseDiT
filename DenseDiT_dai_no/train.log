Using config: /home/users/astar/ares/qianhang/scratch/chengyou/sx/DenseDiT/DenseDiT_dai_no/train/config/config_densedit_stage2.yaml
DeepSpeedStrategy is availableDeepSpeedStrategy is available
DeepSpeedStrategy is available
DeepSpeedStrategy is available

DeepSpeed version: 0.17.6DeepSpeed version: 0.17.6DeepSpeed version: 0.17.6


DeepSpeed version: 0.17.6
Rank: 1, World Size: 4Rank: 2, World Size: 4

Rank: 3, World Size: 4
Dataset length:Dataset length:  5050

Dataset length: 50
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][ALoading pipeline components...:  14%|█▍        | 1/7 [00:00<00:02,  2.05it/s]Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:02,  2.05it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][Awandb: Currently logged in as: xinshen (xinshen-xi-an-jiaotong-university-) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.26it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][Awandb: setting up run hzd451z9
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/wandb/run-20251004_162949-hzd451z9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 20251004-162949
wandb: ⭐️ View project at https://wandb.ai/xinshen-xi-an-jiaotong-university-/DenseDiT
wandb: 🚀 View run at https://wandb.ai/xinshen-xi-an-jiaotong-university-/DenseDiT/runs/hzd451z9
Rank: 0, World Size: 4
Config: {'flux_path': '/home/users/astar/ares/qianhang/scratch/chengyou/hugging/Kontext', 'dtype': 'bfloat16', 'train': {'batch_size': 1, 'accumulate_grad_batches': 1, 'dataloader_workers': 4, 'save_interval': 5000, 'sample_interval': 20, 'max_steps': -1, 'gradient_checkpointing': True, 'save_path': './output', 'optimizer': {'type': 'AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}}, 'save_every_n_steps': 1000, 'log_every_n_steps': 50, 'wandb': {'project': 'DenseDiT', 'enabled': True}}, 'model': {'use_lora': False, 'train_transformer': True}}
Dataset length: 50
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|█████     | 1/2 [00:28<00:28, 28.94s/it][A

Loading checkpoint shards:  50%|█████     | 1/2 [00:28<00:28, 28.39s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.82s/it][A[A
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.74s/it][A
Loading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.12s/it][ALoading checkpoint shards: 100%|██████████| 2/2 [00:56<00:00, 28.24s/it]
Loading pipeline components...:  29%|██▊       | 2/7 [00:57<02:47, 33.45s/it]Loading pipeline components...:  43%|████▎     | 3/7 [00:57<01:12, 18.23s/it]


Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.86s/it][ALoading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.45s/it][ALoading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:57<00:00, 28.66s/it]
[ALoading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.12s/it]
Loading pipeline components...:  43%|████▎     | 3/7 [00:57<01:43, 25.97s/it]Loading pipeline components...:  14%|█▍        | 1/7 [00:57<05:44, 57.42s/it]Loading pipeline components...:  29%|██▊       | 2/7 [00:54<02:15, 27.19s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading pipeline components...:  57%|█████▋    | 4/7 [00:57<00:33, 11.18s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading pipeline components...:  57%|█████▋    | 4/7 [00:54<00:33, 11.25s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading pipeline components...:  57%|█████▋    | 4/7 [01:10<00:33, 11.25s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [01:01<02:02, 61.13s/it][A
Loading checkpoint shards:  33%|███▎      | 1/3 [01:02<02:04, 62.05s/it]
[ALoading checkpoint shards:  33%|███▎      | 1/3 [01:01<02:03, 61.80s/it][A
Loading checkpoint shards:  33%|███▎      | 1/3 [01:01<02:03, 61.97s/it][A
Loading checkpoint shards:  67%|██████▋   | 2/3 [02:01<01:00, 60.31s/it][A

Loading checkpoint shards:  67%|██████▋   | 2/3 [02:02<01:00, 60.90s/it][A
Loading checkpoint shards:  67%|██████▋   | 2/3 [02:02<01:01, 61.06s/it][ALoading checkpoint shards:  67%|██████▋   | 2/3 [02:01<01:00, 60.80s/it][A
Loading checkpoint shards: 100%|██████████| 3/3 [02:25<00:00, 43.78s/it][ALoading checkpoint shards: 100%|██████████| 3/3 [02:25<00:00, 48.48s/it]
Loading pipeline components...:  86%|████████▌ | 6/7 [03:20<00:39, 39.34s/it]


Loading checkpoint shards: 100%|██████████| 3/3 [02:25<00:00, 44.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:26<00:00, 44.00s/it][ALoading checkpoint shards: 100%|██████████| 3/3 [02:26<00:00, 44.08s/it][A[ALoading checkpoint shards: 100%|██████████| 3/3 [02:26<00:00, 48.68s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:25<00:00, 48.65s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:26<00:00, 48.68s/it]
Loading pipeline components...:  86%|████████▌ | 6/7 [03:23<00:43, 43.95s/it]Loading pipeline components...:  57%|█████▋    | 4/7 [03:23<03:40, 73.39s/it]Loading pipeline components...:  29%|██▊       | 2/7 [03:23<09:07, 109.58s/it]Loading pipeline components...: 100%|██████████| 7/7 [03:23<00:00, 29.09s/it]
Loading pipeline components...: 100%|██████████| 7/7 [03:21<00:00, 29.98s/it]Loading pipeline components...:  43%|████▎     | 3/7 [03:24<03:59, 59.96s/it] Loading pipeline components...:  71%|███████▏  | 5/7 [03:24<01:34, 47.26s/it]Loading pipeline components...: 100%|██████████| 7/7 [03:21<00:00, 28.77s/it]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading pipeline components...:  86%|████████▌ | 6/7 [03:24<00:31, 31.23s/it]Loading pipeline components...: 100%|██████████| 7/7 [03:24<00:00, 29.23s/it]
Loading pipeline components...:  57%|█████▋    | 4/7 [03:24<01:49, 36.46s/it]Loading pipeline components...: 100%|██████████| 7/7 [03:25<00:00, 13.76s/it]Loading pipeline components...: 100%|██████████| 7/7 [03:25<00:00, 29.41s/it]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
Traceback (most recent call last):
  File "<frozen runpy>", line 88, in _run_code
Traceback (most recent call last):
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 300, in <module>
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 300, in <module>
  File "<frozen runpy>", line 88, in _run_code
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 300, in <module>
        main()main()

  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 130, in main
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 130, in main
    main()
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 130, in main
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    module.to(device, dtype)
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    return super().to(*args, **kwargs)
    return super().to(*args, **kwargs)    
return super().to(*args, **kwargs)
                 ^  ^  ^    ^   ^  ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
^^
^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
            return self._apply(convert)return self._apply(convert)return self._apply(convert)


                              ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
^

  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
Traceback (most recent call last):
    module._apply(fn)
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
    module._apply(fn)
    module._apply(fn)  [Previous line repeated 1 more time]

  [Previous line repeated 1 more time]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
  [Previous line repeated 2 more times]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^    
param_applied = fn(param)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
                    ^^^^^^^^^
    param_applied = fn(param)  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert

                    ^^^^    ^return t.to(^
^^^   File "<frozen runpy>", line 198, in _run_module_as_main

      File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
      ^^^^^
torch.    OutOfMemoryErrorreturn t.to(: 
CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 11.06 MiB is free. Process 4038039 has 22.22 GiB memory in use. Process 4038037 has 18.42 GiB memory in use. Process 4038038 has 20.25 GiB memory in use. Including non-PyTorch memory, this process has 18.16 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 5.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) 
          ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 11.06 MiB is free. Process 4038039 has 22.22 GiB memory in use. Process 4038037 has 18.42 GiB memory in use. Including non-PyTorch memory, this process has 20.25 GiB memory in use. Process 4038040 has 18.16 GiB memory in use. Of the allocated memory 19.74 GiB is allocated by PyTorch, and 6.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "<frozen runpy>", line 88, in _run_code
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 11.06 MiB is free. Including non-PyTorch memory, this process has 22.22 GiB memory in use. Process 4038037 has 18.42 GiB memory in use. Process 4038038 has 20.25 GiB memory in use. Process 4038040 has 18.16 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 6.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 300, in <module>
    main()
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 130, in main
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 11.06 MiB is free. Process 4038039 has 22.22 GiB memory in use. Including non-PyTorch memory, this process has 18.42 GiB memory in use. Process 4038038 has 20.25 GiB memory in use. Process 4038040 has 18.16 GiB memory in use. Of the allocated memory 17.91 GiB is allocated by PyTorch, and 5.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33m20251004-162949[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../../../../../../../scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/wandb/run-20251004_162949-hzd451z9/logs[0m
W1004 16:33:50.624000 4037946 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 4038037 closing signal SIGTERM
W1004 16:33:50.625000 4037946 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 4038038 closing signal SIGTERM
W1004 16:33:50.626000 4037946 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 4038039 closing signal SIGTERM
E1004 16:33:52.042000 4037946 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 3 (pid: 4038040) of binary: /home/users/astar/ares/qianhang/.conda/envs/omini_sx/bin/python3.12
Traceback (most recent call last):
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src.train.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-04_16:33:50
  host      : a2ap-dgx035.asp2p.nscc.sg
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4038040)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
