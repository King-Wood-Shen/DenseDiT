Rank: 0, World Size: 4
Config: {'flux_path': '/home/users/astar/ares/qianhang/scratch/chengyou/hugging/Kontext', 'dtype': 'bfloat16', 'train': {'batch_size': 4, 'accumulate_grad_batches': 4, 'dataloader_workers': 8, 'max_steps': 100000, 'max_epochs': -1, 'gradient_clip_val': 0.5, 'gradient_checkpointing': True, 'optimizer': {'type': 'AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}}, 'save_path': './output', 'save_every_n_steps': 1000, 'log_every_n_steps': 50, 'wandb': {'project': 'DenseDiT', 'enabled': True}}, 'model': {'use_lora': False, 'train_transformer': True}}
Dataset length: 50
Loading checkpoint shards: 100%|██████████| 3/3 [02:09<00:00, 43.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.85s/it]9s/it]
Loading pipeline components...:  43%|████▎     | 3/7 [02:37<02:55, 43.92s/it] You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading pipeline components...: 100%|██████████| 7/7 [02:37<00:00, 22.56s/it]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 201, in <module>
    main()
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 113, in main
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 83.06 MiB is free. Process 3683854 has 14.87 GiB memory in use. Including non-PyTorch memory, this process has 15.50 GiB memory in use. Process 3683855 has 31.95 GiB memory in use. Process 3683853 has 16.66 GiB memory in use. Of the allocated memory 14.99 GiB is allocated by PyTorch, and 6.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
