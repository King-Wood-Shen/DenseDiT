Rank: 0, World Size: 4
Config: {'flux_path': '/home/users/astar/ares/qianhang/scratch/chengyou/hugging/Kontext', 'dtype': 'bfloat16', 'train': {'batch_size': 1, 'accumulate_grad_batches': 1, 'dataloader_workers': 4, 'save_interval': 5000, 'sample_interval': 20, 'max_steps': -1, 'gradient_checkpointing': True, 'save_path': './output', 'optimizer': {'type': 'AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}}, 'save_every_n_steps': 1000, 'log_every_n_steps': 50, 'wandb': {'project': 'DenseDiT', 'enabled': True}}, 'model': {'use_lora': False, 'train_transformer': True}}
Dataset length: 50
Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.51s/it]
Loading pipeline components...:  43%|████▎     | 3/7 [00:21<00:22,  5.63s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]s/it]
Loading pipeline components...: 100%|██████████| 7/7 [00:27<00:00,  3.97s/it]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 201, in <module>
    main()
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 113, in main
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 13.06 MiB is free. Including non-PyTorch memory, this process has 18.88 GiB memory in use. Process 3715255 has 22.52 GiB memory in use. Process 3715253 has 18.25 GiB memory in use. Process 3715254 has 19.41 GiB memory in use. Of the allocated memory 18.37 GiB is allocated by PyTorch, and 5.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
