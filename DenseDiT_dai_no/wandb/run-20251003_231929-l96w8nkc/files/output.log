Rank: 0, World Size: 4
Config: {'flux_path': '/home/users/astar/ares/qianhang/scratch/chengyou/hugging/Kontext', 'dtype': 'bfloat16', 'train': {'batch_size': 4, 'accumulate_grad_batches': 4, 'dataloader_workers': 8, 'max_steps': 100000, 'max_epochs': -1, 'gradient_clip_val': 0.5, 'gradient_checkpointing': True, 'optimizer': {'type': 'AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}}, 'save_path': './output', 'save_every_n_steps': 1000, 'log_every_n_steps': 50, 'wandb': {'project': 'DenseDiT', 'enabled': True}}, 'model': {'use_lora': False, 'train_transformer': True}}
Dataset length: 50
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]it/s]
Loading pipeline components...:  57%|█████▋    | 4/7 [00:10<00:09,  3.22s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 30.96s/it]s/it]
Loading pipeline components...: 100%|██████████| 7/7 [01:43<00:00, 14.75s/it]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 201, in <module>
    main()
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/train.py", line 113, in main
    trainable_model = DenseDiTModel(
                      ^^^^^^^^^^^^^^
  File "/scratch/users/astar/ares/qianhang/chengyou/sx/DenseDiT/DenseDiT_dai_no/src/train/model.py", line 32, in __init__
    FluxKontextPipeline.from_pretrained(flux_pipe_id).to(dtype=dtype).to(device)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py", line 541, in to
    module.to(device, dtype)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/diffusers/models/modeling_utils.py", line 1424, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/users/astar/ares/qianhang/.conda/envs/omini_sx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 65.06 MiB is free. Process 3701349 has 16.33 GiB memory in use. Including non-PyTorch memory, this process has 20.39 GiB memory in use. Process 3701351 has 22.47 GiB memory in use. Process 3701350 has 19.83 GiB memory in use. Of the allocated memory 19.88 GiB is allocated by PyTorch, and 6.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
